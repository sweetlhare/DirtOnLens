{"cells":[{"cell_type":"markdown","source":["# Gdrive"],"metadata":{"id":"zrSO4TK4HIsR"},"id":"zrSO4TK4HIsR"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxRczVjIKBV7","executionInfo":{"status":"ok","timestamp":1733625776239,"user_tz":-540,"elapsed":16860,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"9753680e-cd21-49f8-f52b-a59a76a4f05e"},"id":"fxRczVjIKBV7","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["# !unzip /gdrive/My\\ Drive/DIRT/full_new_data.zip -d /gdrive/My\\ Drive/DIRT/"],"metadata":{"id":"XQ8B7YYumsoa","executionInfo":{"status":"ok","timestamp":1733625776239,"user_tz":-540,"elapsed":7,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"XQ8B7YYumsoa","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# !pip install ultralytics"],"metadata":{"id":"AdqG596IKhkX","executionInfo":{"status":"ok","timestamp":1733625776239,"user_tz":-540,"elapsed":6,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"collapsed":true},"id":"AdqG596IKhkX","execution_count":3,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdR9RMGgKi-N","executionInfo":{"status":"ok","timestamp":1733625776239,"user_tz":-540,"elapsed":6,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"2d800671-dcc0-41dd-dcf0-c36e92d040dc"},"id":"SdR9RMGgKi-N","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec  8 02:42:55 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   46C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"mjllkrmdHMk5"},"id":"mjllkrmdHMk5"},{"cell_type":"code","source":["# !pip install albumentations wandb"],"metadata":{"id":"LHWyz8F-4i-Z","executionInfo":{"status":"ok","timestamp":1733600997736,"user_tz":-540,"elapsed":3,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"LHWyz8F-4i-Z","execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","import glob\n","import math\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from torch.cuda.amp import autocast, GradScaler\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rr-7yL3rHOnN","executionInfo":{"status":"ok","timestamp":1733626033688,"user_tz":-540,"elapsed":4458,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"8f7fdf36-e33a-4a9e-a7ed-8b72ea14254a"},"id":"rr-7yL3rHOnN","execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"gzwVX8inHRef"},"id":"gzwVX8inHRef"},{"cell_type":"code","source":["# Configuration\n","class Config:\n","    # Data\n","    IMAGE_SIZE = 256  # Changed from 384 to 256\n","    BATCH_SIZE = 32\n","    NUM_WORKERS = 4\n","\n","    # Model\n","    IN_CHANNELS = 3\n","    NUM_CLASSES = 1\n","\n","    # Training\n","    EPOCHS = 300  # Changed from 100 to 300\n","    LEARNING_RATE = 1e-3  # Confirmed correct\n","    MIN_LEARNING_RATE = 1e-6  # Added minimum learning rate\n","    WEIGHT_DECAY = 0.9  # Changed to match paper's momentum decay\n","    GRADIENT_CLIP = 1.0\n","    AUGMENTATION_STOP_EPOCH = 180  # New parameter"],"metadata":{"id":"7kXYMF5rHQtQ","executionInfo":{"status":"ok","timestamp":1733626033689,"user_tz":-540,"elapsed":4,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"7kXYMF5rHQtQ","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"qNfo2-InHWfI"},"id":"qNfo2-InHWfI"},{"cell_type":"code","source":["# # Model Components from Original Code\n","# def autopad(k, p=None, d=1):\n","#     if d > 1:\n","#         k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]\n","#     if p is None:\n","#         p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n","#     return p\n","\n","# class Conv(nn.Module):\n","#     default_act = nn.GELU()\n","\n","#     def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n","#         super().__init__()\n","#         self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n","#         self.bn = nn.BatchNorm2d(c2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","#         self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n","\n","#     def forward(self, x):\n","#         return self.act(self.bn(self.conv(x)))\n","\n","#     def forward_fuse(self, x):\n","#         return self.act(self.conv(x))\n","\n","# class DWConv(Conv):\n","#     def __init__(self, c1, c2, k=1, s=1, d=1, act=True):\n","#         super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n","\n","# # Enhanced CMRF with SE block\n","# class SEBlock(nn.Module):\n","#     def __init__(self, channel, reduction=16):\n","#         super().__init__()\n","#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","#         self.fc = nn.Sequential(\n","#             nn.Linear(channel, channel // reduction, bias=False),\n","#             nn.ReLU(inplace=True),\n","#             nn.Linear(channel // reduction, channel, bias=False),\n","#             nn.Sigmoid()\n","#         )\n","\n","#     def forward(self, x):\n","#         b, c, _, _ = x.size()\n","#         y = self.avg_pool(x).view(b, c)\n","#         y = self.fc(y).view(b, c, 1, 1)\n","#         return x * y.expand_as(x)\n","\n","# class CMRF(nn.Module):\n","#     def __init__(self, c1, c2, N=8, shortcut=True, g=1, e=0.5):\n","#         super().__init__()\n","#         self.N = N\n","#         self.c = int(c2 * e / self.N)\n","#         self.add = shortcut and c1 == c2\n","\n","#         self.pwconv1 = Conv(c1, c2 // self.N, 1, 1)\n","#         self.pwconv2 = Conv(c2 // 2, c2, 1, 1)\n","#         self.m = nn.ModuleList(DWConv(self.c, self.c, k=3, act=False) for _ in range(N - 1))\n","\n","#         # Added SE block and dropout\n","#         self.se = SEBlock(c2)\n","#         self.dropout = nn.Dropout2d(0.1)\n","\n","#     def forward(self, x):\n","#         x_residual = x\n","#         x = self.pwconv1(x)\n","#         x = [x[:, 0::2, :, :], x[:, 1::2, :, :]]\n","#         x.extend(m(x[-1]) for m in self.m)\n","#         x[0] = x[0] + x[1]\n","#         x.pop(1)\n","#         y = torch.cat(x, dim=1)\n","#         y = self.pwconv2(y)\n","#         y = self.se(y)\n","#         y = self.dropout(y)\n","#         return x_residual + y if self.add else y\n","\n","\n","# class UNetEncoder(nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super(UNetEncoder, self).__init__()\n","#         self.cmrf = CMRF(in_channels, out_channels)\n","#         self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","#     def forward(self, x):\n","#         x = self.cmrf(x)\n","#         return self.downsample(x), x\n","\n","# class UNetDecoder(nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super(UNetDecoder, self).__init__()\n","#         self.cmrf = CMRF(in_channels, out_channels)\n","\n","#     def forward(self, x, skip_connection):\n","#         x = F.interpolate(x, scale_factor=2, mode='bicubic', align_corners=False)\n","#         x = torch.cat([x, skip_connection], dim=1)\n","#         x = self.cmrf(x)\n","#         return x\n","\n","# class TinyUNet(nn.Module):\n","#     def __init__(self, in_channels=3, num_classes=1):\n","#         super(TinyUNet, self).__init__()\n","#         in_filters = [192, 384, 768, 1024]\n","#         out_filters = [64, 128, 256, 512]\n","\n","#         self.encoder1 = UNetEncoder(in_channels, 64)\n","#         self.encoder2 = UNetEncoder(64, 128)\n","#         self.encoder3 = UNetEncoder(128, 256)\n","#         self.encoder4 = UNetEncoder(256, 512)\n","#         self.decoder4 = UNetDecoder(in_filters[3], out_filters[3])\n","#         self.decoder3 = UNetDecoder(in_filters[2], out_filters[2])\n","#         self.decoder2 = UNetDecoder(in_filters[1], out_filters[1])\n","#         self.decoder1 = UNetDecoder(in_filters[0], out_filters[0])\n","#         self.final_conv = nn.Conv2d(out_filters[0], num_classes, kernel_size=1)\n","\n","#     def forward(self, x):\n","#         x, skip1 = self.encoder1(x)\n","#         x, skip2 = self.encoder2(x)\n","#         x, skip3 = self.encoder3(x)\n","#         x, skip4 = self.encoder4(x)\n","#         x = self.decoder4(x, skip4)\n","#         x = self.decoder3(x, skip3)\n","#         x = self.decoder2(x, skip2)\n","#         x = self.decoder1(x, skip1)\n","#         x = self.final_conv(x)\n","#         return x"],"metadata":{"id":"QhPDLGlPHYIA","executionInfo":{"status":"ok","timestamp":1733625790033,"user_tz":-540,"elapsed":537,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"QhPDLGlPHYIA","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"nyzWkD0jHacl"},"id":"nyzWkD0jHacl"},{"cell_type":"code","source":["# Enhanced Dataset with Augmentations\n","class SegmentationDataset(Dataset):\n","    def __init__(self, image_dirs, mask_dirs, transform=None, is_training=True, current_epoch=0):\n","        self.image_paths = []\n","        self.mask_paths = []\n","        self.is_training = is_training\n","        self.current_epoch = current_epoch\n","\n","        # Get image and mask paths\n","        for img_dir, msk_dir in zip(image_dirs, mask_dirs):\n","            img_paths = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n","            for img_path in img_paths:\n","                filename = os.path.basename(img_path).replace('.jpg', '.png')\n","                mask_path = os.path.join(msk_dir, filename)\n","                if os.path.exists(mask_path):\n","                    self.image_paths.append(img_path)\n","                    self.mask_paths.append(mask_path)\n","\n","        # Training transformations\n","        self.train_transform = A.Compose([\n","            A.RandomResizedCrop(256, 256, scale=(0.8, 1.0)),\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.5),\n","            A.RandomRotate90(p=0.5),\n","            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.5),\n","            A.OneOf([\n","                A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n","                A.GridDistortion(p=0.5),\n","                A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=0.5)\n","            ], p=0.3),\n","            A.OneOf([\n","                A.GaussNoise(p=0.5),\n","                A.RandomBrightnessContrast(p=0.5),\n","                A.RandomGamma(p=0.5)\n","            ], p=0.3),\n","            ToTensorV2()\n","        ])\n","\n","        # Validation transformations\n","        self.val_transform = A.Compose([\n","            A.Resize(256, 256),  # Changed from 384 to 256\n","            ToTensorV2()\n","        ])\n","\n","    def __getitem__(self, idx):\n","        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n","        mask = np.array(Image.open(self.mask_paths[idx]).convert('L'))\n","\n","        # Apply transformations based on training phase and epoch\n","        if self.is_training and self.current_epoch < Config.AUGMENTATION_STOP_EPOCH:\n","            transformed = self.train_transform(image=image, mask=mask)\n","        else:\n","            transformed = self.val_transform(image=image, mask=mask)\n","\n","        image = transformed['image'].float() / 255.0\n","        mask = transformed['mask'].float().unsqueeze(0) / 255.0\n","\n","        return image, mask\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def update_epoch(self, epoch):\n","        self.current_epoch = epoch"],"metadata":{"collapsed":true,"id":"OIj04ljA2xaN","executionInfo":{"status":"ok","timestamp":1733626034508,"user_tz":-540,"elapsed":2,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"OIj04ljA2xaN","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Loss"],"metadata":{"id":"w5hee_ZhIMk9"},"id":"w5hee_ZhIMk9"},{"cell_type":"code","source":["class BCEJaccardLoss(nn.Module):\n","    def __init__(self, mode='binary', smooth=1.0, eps=1e-7, from_logits=True):\n","        super(BCEJaccardLoss, self).__init__()\n","        self.mode = mode\n","        self.smooth = smooth\n","        self.eps = eps\n","        self.from_logits = from_logits\n","\n","    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n","        assert y_true.size(0) == y_pred.size(0)\n","\n","        if self.from_logits:\n","            y_pred = torch.sigmoid(y_pred)\n","\n","        bs = y_true.size(0)\n","\n","        y_true = y_true.view(bs, -1)\n","        y_pred = y_pred.view(bs, -1)\n","\n","        intersection = torch.sum(y_true * y_pred, dim=1)\n","        sum_ = torch.sum(y_true + y_pred, dim=1)\n","        jac = (intersection + self.smooth) / (sum_ - intersection + self.smooth)\n","\n","        bce = nn.functional.binary_cross_entropy(y_pred, y_true, reduction='none')\n","        bce = torch.mean(bce, dim=1)\n","\n","        loss = (1 - jac) * self.smooth + bce\n","\n","        return loss.mean()\n"],"metadata":{"id":"o17T32r82rro","executionInfo":{"status":"ok","timestamp":1733626034801,"user_tz":-540,"elapsed":2,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"o17T32r82rro","execution_count":4,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"apF3bZ4D2ruG","executionInfo":{"status":"ok","timestamp":1733625792931,"user_tz":-540,"elapsed":1,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"apF3bZ4D2ruG","execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XJHcOHbE2rwe","executionInfo":{"status":"ok","timestamp":1733625793363,"user_tz":-540,"elapsed":1,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"XJHcOHbE2rwe","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"zYGTT9yKIrfz"},"id":"zYGTT9yKIrfz"},{"cell_type":"code","source":["# Set random seed\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Initialize device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","\n","# Create data loaders\n","train_image_dirs = ['/gdrive/My Drive/DIRT/full_new_data/img']\n","train_mask_dirs = ['/gdrive/My Drive/DIRT/full_new_data/msk']\n","val_image_dirs = ['/gdrive/My Drive/DIRT/cv_open_dataset/open_img']\n","val_mask_dirs = ['/gdrive/My Drive/DIRT/cv_open_dataset/open_msk']\n","# Initialize datasets\n","train_dataset = SegmentationDataset(\n","    train_image_dirs,\n","    train_mask_dirs,\n","    is_training=True\n",")\n","\n","val_dataset = SegmentationDataset(\n","    val_image_dirs,\n","    val_mask_dirs,\n","    is_training=False\n",")\n","# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=Config.BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=Config.NUM_WORKERS,\n","    pin_memory=True\n",")\n","val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=Config.BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=Config.NUM_WORKERS,\n","        pin_memory=True\n","    )\n","\n","print(f'Training dataset size: {len(train_dataset)}')\n","print(f'Validation dataset size: {len(val_dataset)}')\n","\n","# # Initialize model\n","# model = TinyUNet(\n","#     in_channels=Config.IN_CHANNELS,\n","#     num_classes=Config.NUM_CLASSES\n","# ).to(device)\n","\n","def display_sample_prediction(model, val_loader, device):\n","    model.eval()\n","    with torch.no_grad():\n","        sample_image, sample_mask = next(iter(val_loader))\n","        sample_image = sample_image[0:1].float().to(device)  # явно указываем float\n","        sample_mask = sample_mask[0:1].float()  # явно указываем float\n","        output = model(sample_image)\n","        pred_mask = torch.sigmoid(output).cpu() > 0.5\n","        plt.figure(figsize=(15, 5))\n","\n","        plt.subplot(1, 3, 1)\n","        plt.title('Input Image')\n","        plt.imshow(sample_image[0].cpu().permute(1, 2, 0))\n","        plt.axis('off')\n","\n","        plt.subplot(1, 3, 2)\n","        plt.title('True Mask')\n","        plt.imshow(sample_mask[0, 0], cmap='gray')\n","        plt.axis('off')\n","\n","        plt.subplot(1, 3, 3)\n","        plt.title('Predicted Mask')\n","        plt.imshow(pred_mask[0, 0], cmap='gray')\n","        plt.axis('off')\n","\n","        plt.savefig('sample_prediction.png')\n","        plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ZigLCRT2ry2","executionInfo":{"status":"ok","timestamp":1733626046332,"user_tz":-540,"elapsed":629,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"e9cfee8a-352c-478f-8f28-c19b450d23e9"},"id":"0ZigLCRT2ry2","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training dataset size: 2282\n","Validation dataset size: 246\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-1416174b9817>:27: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n","  A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n"]}]},{"cell_type":"code","source":["# !pip install segmentation-models-pytorch"],"metadata":{"collapsed":true,"id":"UPmgcCKhiMxH","executionInfo":{"status":"ok","timestamp":1733626051071,"user_tz":-540,"elapsed":460,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"UPmgcCKhiMxH","execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Function to visualize predictions\n","def visualize_epoch_results(model, val_loader, epoch, device):\n","    # Only visualize every 4 epochs\n","    if epoch % 4 != 0:\n","        return\n","\n","    model.eval()\n","    with torch.no_grad():\n","        # Get a single batch\n","        images, masks = next(iter(val_loader))\n","        # Take first image from batch\n","        image = images[0:1].to(device)\n","        mask = masks[0:1]\n","\n","        # Get prediction\n","        output = model(image)\n","        pred_mask = torch.sigmoid(output).cpu() > 0.5\n","\n","        # Create figure\n","        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        # Plot original image\n","        axes[0].imshow(images[0].permute(1, 2, 0).cpu())\n","        axes[0].set_title('Original Image')\n","        axes[0].axis('off')\n","\n","        # Plot true mask\n","        axes[1].imshow(mask[0, 0].cpu(), cmap='gray')\n","        axes[1].set_title('True Mask')\n","        axes[1].axis('off')\n","\n","        # Plot predicted mask\n","        axes[2].imshow(pred_mask[0, 0].cpu(), cmap='gray')\n","        axes[2].set_title('Predicted Mask')\n","        axes[2].axis('off')\n","\n","        plt.suptitle(f'Epoch {epoch+1}')\n","\n","        # Save the figure\n","        plt.savefig(f'epoch_{epoch+1}_prediction.png')\n","        plt.show()\n","        plt.close()\n","\n","        # Log to wandb only every 4 epochs\n","        wandb.log({\n","            \"predictions\": wandb.Image(f'epoch_{epoch+1}_prediction.png')\n","        })\n","\n","def calculate_metrics(pred, target, threshold=0.5):\n","    \"\"\"\n","    Вычисляет IoU для батча\n","    pred: тензор предсказаний после sigmoid (B, 1, H, W)\n","    target: тензор истинных масок (B, 1, H, W)\n","    \"\"\"\n","    # Применяем threshold к предсказаниям\n","    pred = (pred > threshold).float()\n","\n","    # Вычисляем IoU для каждого изображения в батче\n","    intersection = (pred * target).sum(dim=(2, 3))  # (B, 1)\n","    union = (pred + target).gt(0).float().sum(dim=(2, 3))  # (B, 1)\n","\n","    # Добавляем малое число для численной стабильности\n","    iou = (intersection + 1e-8) / (union + 1e-8)  # (B, 1)\n","\n","    return iou.mean(dim=1)  # Среднее по каналам для каждого изображения\n","\n","\n","# Rest of the training loop remains the same\n","wandb.init(project=\"segmentation-project-3\")\n","\n","import segmentation_models_pytorch as smp\n","\n","model = smp.Unet(\n","    encoder_name=\"mobileone_s1\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n","    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","    classes=1,                      # model output channels (number of classes in your dataset)\n",").to(device)\n","\n","# model = TinyUNet(in_channels=Config.IN_CHANNELS, num_classes=Config.NUM_CLASSES).to(device)\n","\n","# # Load checkpoint\n","# checkpoint = torch.load('best_model.pt')\n","# # Load model state\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","\n","model = model.to(memory_format=torch.channels_last)\n","\n","criterion = BCEJaccardLoss().to(device)\n","\n","optimizer = optim.Adam(\n","    model.parameters(),\n","    lr=Config.LEARNING_RATE,\n","    # lr=1e-3,\n","    betas=(Config.WEIGHT_DECAY, 0.999)  # First beta set to 0.9 as per paper\n",")\n","\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","    optimizer,\n","    T_max=Config.EPOCHS,\n","    eta_min=Config.MIN_LEARNING_RATE\n",")\n","\n","scaler = torch.amp.GradScaler()\n","best_iou = 0\n","patience = 100\n","no_improve = 0\n","\n","for epoch in range(Config.EPOCHS):\n","    # TRAIN\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{Config.EPOCHS}'):\n","        images = images.to(device)\n","        masks = masks.to(device)\n","\n","        # with torch.amp.autocast(device_type='cuda'):\n","        outputs = model(images)\n","        loss = criterion(outputs, masks)\n","\n","        scaler.scale(loss).backward()\n","\n","        scaler.step(optimizer)\n","        scaler.update()\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    train_loss = running_loss / len(train_loader.dataset)\n","\n","    # VALIDATE\n","    model.eval()\n","    running_loss = 0.0\n","    val_ious = []  # Список для хранения IoU каждого изображения\n","\n","    with torch.no_grad():\n","        for images, masks in val_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            # with torch.amp.autocast(device_type='cuda'):\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            # Получаем sigmoid от выходов\n","            outputs = torch.sigmoid(outputs)\n","\n","            # Считаем IoU для текущего батча\n","            batch_ious = calculate_metrics(outputs, masks)\n","            val_ious.extend(batch_ious.cpu().numpy())\n","\n","            running_loss += loss.item() * images.size(0)\n","\n","    # Считаем средние метрики\n","    val_loss = running_loss / len(val_loader.dataset)\n","    val_miou = np.mean(val_ious)  # Среднее IoU по всем изображениям\n","\n","    current_lr = optimizer.param_groups[0]['lr']\n","    scheduler.step()\n","\n","    # Визуализация\n","    visualize_epoch_results(model, val_loader, epoch, device)\n","\n","    # Логируем метрики\n","    wandb.log({\n","        'train_loss': train_loss,\n","        'val_loss': val_loss,\n","        'val_miou': val_miou,\n","        'learning_rate': current_lr\n","    })\n","\n","    # Выводим метрики\n","    print(f'Epoch {epoch+1}/{Config.EPOCHS}')\n","    print(f'Learning Rate: {current_lr:.6f}')\n","    print(f'Train Loss: {train_loss:.4f}')\n","    print(f'Val Loss: {val_loss:.4f}, Val mIoU: {val_miou:.4f}')\n","\n","    # Сохраняем лучшую модель\n","    if val_miou > best_iou:\n","        best_iou = val_miou\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'best_miou': best_iou,\n","        }, 'best_model.pt')\n","        print(f'Saved new best model with mIoU: {val_miou:.4f}')\n","        no_improve = 0\n","    else:\n","        no_improve += 1\n","\n","    if no_improve >= patience:\n","        print(f'Early stopping triggered after {patience} epochs without improvement')\n","        break\n","\n","# Cleanup\n","wandb.finish()\n","print(\"\\nTraining completed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"11ws9EN9BFAKlrnjRFKLpnYl9RBmculEc"},"collapsed":true,"id":"OKhrpNIq2r2X","executionInfo":{"status":"ok","timestamp":1733632275807,"user_tz":-540,"elapsed":6086341,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"6899971e-c119-4713-b868-f992f26dacf1"},"id":"OKhrpNIq2r2X","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["!pip install onnx onnxruntime onnxsim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrGqaXqNArDv","executionInfo":{"status":"ok","timestamp":1733632423255,"user_tz":-540,"elapsed":7323,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"55ba6e81-d06c-41c9-f90b-2214d1d09a57"},"id":"rrGqaXqNArDv","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting onnxsim\n","  Downloading onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnxsim) (13.9.4)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim) (2.18.0)\n","Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim) (4.12.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n","Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxsim, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.20.1 onnxsim-0.4.36\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.onnx\n","import onnx\n","import onnxruntime\n","import numpy as np\n","import warnings\n","from pathlib import Path\n","import segmentation_models_pytorch as smp\n","\n","def load_model_safe(model, checkpoint_path):\n","    \"\"\"\n","    Safely load model weights with proper error handling\n","    \"\"\"\n","    try:\n","        # Добавляем numpy.core.multiarray.scalar в список безопасных глобальных объектов\n","        from numpy.core.multiarray import scalar\n","        torch.serialization.add_safe_globals([scalar])\n","\n","        # Пробуем загрузить с weights_only=True\n","        try:\n","            checkpoint = torch.load(checkpoint_path, weights_only=True)\n","        except Exception:\n","            # Если не получилось, загружаем без ограничений\n","            print(\"Warning: Loading checkpoint without weights_only restriction\")\n","            checkpoint = torch.load(checkpoint_path, weights_only=False)\n","\n","        if 'model_state_dict' in checkpoint:\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","        else:\n","            model.load_state_dict(checkpoint)\n","\n","        print(\"Model weights loaded successfully\")\n","\n","    except Exception as e:\n","        print(f\"Error loading model weights: {str(e)}\")\n","        raise\n","\n","    return model.eval()  # Сразу переводим модель в режим eval\n","\n","\n","def export_smp_model_to_onnx(model,\n","                            path='model.onnx',\n","                            input_shape=(1, 3, 256, 256),\n","                            simplify=True):\n","    \"\"\"\n","    Export segmentation model to ONNX format with additional optimizations\n","    \"\"\"\n","    path = Path(path)\n","    device = next(model.parameters()).device\n","    dummy_input = torch.randn(input_shape, device=device)\n","\n","    with warnings.catch_warnings():\n","        warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n","        warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","        try:\n","            # Export model with additional settings\n","            torch.onnx.export(\n","                model,\n","                dummy_input,\n","                path,\n","                export_params=True,\n","                opset_version=13,\n","                do_constant_folding=True,\n","                input_names=['input'],\n","                output_names=['output'],\n","                dynamic_axes={\n","                    'input': {0: 'batch_size'},\n","                    'output': {0: 'batch_size'}\n","                }\n","            )\n","\n","            # Verify ONNX model\n","            onnx_model = onnx.load(path)\n","            onnx.checker.check_model(onnx_model)\n","\n","            # Optimize model if requested\n","            if simplify:\n","                try:\n","                    import onnxsim\n","                    model_simplified, check = onnxsim.simplify(onnx_model)\n","                    if check:\n","                        onnx.save(model_simplified, path)\n","                        print(\"Model simplified successfully\")\n","                except ImportError:\n","                    print(\"onnx-simplifier not installed. Skip simplification.\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"Error during model export: {str(e)}\")\n","            return False\n","\n","def verify_onnx_model(model, onnx_path, input_shape=(1, 3, 256, 256), rtol=1e-3, atol=1e-4):\n","    \"\"\"\n","    Verify ONNX model output matches PyTorch model with better tolerance\n","    \"\"\"\n","    device = next(model.parameters()).device\n","\n","    try:\n","        # Load and check ONNX model\n","        onnx_model = onnx.load(onnx_path)\n","        onnx.checker.check_model(onnx_model)\n","\n","        # Create random input with seed for reproducibility\n","        torch.manual_seed(42)\n","        x = torch.randn(input_shape, device=device)\n","\n","        # PyTorch prediction\n","        with torch.no_grad():\n","            torch_out = model(x)\n","\n","        # ONNX Runtime prediction\n","        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n","        ort_session = onnxruntime.InferenceSession(onnx_path, providers=providers)\n","        ort_inputs = {ort_session.get_inputs()[0].name: x.cpu().numpy()}\n","        ort_out = ort_session.run(None, ort_inputs)[0]\n","\n","        # Compare outputs with relaxed tolerance\n","        np.testing.assert_allclose(\n","            torch_out.cpu().numpy(),\n","            ort_out,\n","            rtol=rtol,\n","            atol=atol,\n","            err_msg=\"Output mismatch between PyTorch and ONNX\"\n","        )\n","        print(\"Exported model has been verified!\")\n","        return True\n","\n","    except Exception as e:\n","        print(f\"Verification failed: {str(e)}\")\n","        return False\n","\n","def main():\n","    # Initialize model and device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # Create model\n","    model = smp.Unet(\n","        encoder_name=\"mobileone_s1\",\n","        # encoder_name=\"efficientnet-b0\",\n","        encoder_weights=\"imagenet\",\n","        in_channels=3,\n","        classes=1\n","    ).to(device)\n","\n","    # Load weights\n","    model = load_model_safe(model, 'best_model.pt')\n","\n","    # Export to ONNX\n","    input_shape = (1, 3, 256, 256)\n","    success = export_smp_model_to_onnx(\n","        model,\n","        path='segmentation_model2.onnx',\n","        input_shape=input_shape\n","    )\n","\n","    if success:\n","        print(\"Model exported successfully!\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nohGybRIA7J5","executionInfo":{"status":"ok","timestamp":1733632427565,"user_tz":-540,"elapsed":4320,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"f76c1494-5111-4651-c1b5-0d1e6ece214a"},"id":"nohGybRIA7J5","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Warning: Loading checkpoint without weights_only restriction\n","Model weights loaded successfully\n","Model simplified successfully\n","Model exported successfully!\n"]}]},{"cell_type":"code","source":["# import torch\n","# import torch.onnx\n","# import onnx\n","# import onnxruntime\n","\n","# def export_to_onnx(model, save_path='model.onnx', input_size=(1, 3, 256, 256)):\n","#     \"\"\"\n","#     Export PyTorch model to ONNX format\n","#     Args:\n","#         model: PyTorch model\n","#         save_path: Path to save ONNX model\n","#         input_size: Input tensor size (batch_size, channels, height, width)\n","#     \"\"\"\n","#     # Set model to evaluation mode\n","#     model.eval()\n","\n","#     # Create dummy input tensor\n","#     dummy_input = torch.randn(input_size, requires_grad=True)\n","\n","#     # Export the model\n","#     torch.onnx.export(\n","#         model,                                      # model being run\n","#         dummy_input,                                # model input (or a tuple for multiple inputs)\n","#         save_path,                                  # where to save the model\n","#         export_params=True,                         # store the trained parameter weights inside the model file\n","#         opset_version=11,                          # the ONNX version to export the model to\n","#         do_constant_folding=True,                   # whether to execute constant folding for optimization\n","#         input_names=['input'],                      # the model's input names\n","#         output_names=['output'],                    # the model's output names\n","#         dynamic_axes={\n","#             'input': {0: 'batch_size'},            # variable length axes\n","#             'output': {0: 'batch_size'}\n","#         }\n","#     )\n","\n","#     # Verify the exported model\n","#     onnx_model = onnx.load(save_path)\n","#     onnx.checker.check_model(onnx_model)\n","\n","#     return onnx_model\n","\n","# def verify_onnx_output(pytorch_model, onnx_path, input_size=(1, 3, 256, 256)):\n","#     \"\"\"\n","#     Verify ONNX model output matches PyTorch model\n","#     \"\"\"\n","#     # Create random input\n","#     x = torch.randn(input_size)\n","\n","#     # PyTorch forward pass\n","#     pytorch_model.eval()\n","#     with torch.no_grad():\n","#         pytorch_out = pytorch_model(x)\n","\n","#     # ONNX Runtime forward pass\n","#     ort_session = onnxruntime.InferenceSession(onnx_path)\n","#     ort_inputs = {ort_session.get_inputs()[0].name: x.numpy()}\n","#     ort_out = ort_session.run(None, ort_inputs)[0]\n","\n","#     # Compare outputs\n","#     np.testing.assert_allclose(pytorch_out.numpy(), ort_out, rtol=1e-03, atol=1e-05)\n","#     print(\"PyTorch and ONNX Runtime outputs matched!\")\n","\n","# # Example usage\n","# def convert_model_to_onnx():\n","#     # Initialize model\n","#     model = TinyUNet(in_channels=3, num_classes=1)\n","\n","#     # Load trained weights if available\n","#     try:\n","#         model.load_state_dict(torch.load('/content/best_model_tine_0_72.pt')['model_state_dict'])\n","#         print(\"Loaded trained weights\")\n","#     except:\n","#         print(\"Using untrained model\")\n","\n","#     # Export to ONNX\n","#     onnx_path = 'tinyunet.onnx'\n","#     onnx_model = export_to_onnx(model, onnx_path)\n","#     print(f\"Model exported to {onnx_path}\")\n","\n","#     # Verify the exported model\n","#     try:\n","#         verify_onnx_output(model, onnx_path)\n","#     except Exception as e:\n","#         print(f\"Verification failed: {str(e)}\")\n","\n","#     return onnx_path\n","\n","# if __name__ == \"__main__\":\n","#     onnx_path = convert_model_to_onnx()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BG4voLXPpQlo","executionInfo":{"status":"ok","timestamp":1733623485193,"user_tz":-540,"elapsed":5243,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}},"outputId":"918d6a45-b0af-4d3e-c7af-59be4b4f502f"},"id":"BG4voLXPpQlo","execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-23-c760226dfe12>:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('/content/best_model_tine_0_72.pt')['model_state_dict'])\n"]},{"output_type":"stream","name":"stdout","text":["Loaded trained weights\n","Model exported to tinyunet.onnx\n","PyTorch and ONNX Runtime outputs matched!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LT0ENo6upQqM","executionInfo":{"status":"aborted","timestamp":1733601084925,"user_tz":-540,"elapsed":13,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"LT0ENo6upQqM","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ojAz-EeDpQsi","executionInfo":{"status":"aborted","timestamp":1733601084925,"user_tz":-540,"elapsed":12,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"ojAz-EeDpQsi","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rAaPjerppQvD","executionInfo":{"status":"aborted","timestamp":1733601084925,"user_tz":-540,"elapsed":11,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"rAaPjerppQvD","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GCkbVL9ypQxe","executionInfo":{"status":"aborted","timestamp":1733601084925,"user_tz":-540,"elapsed":10,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"GCkbVL9ypQxe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cleanup\n","if wandb.run is not None:\n","    wandb.finish()\n","\n","print(\"Cleaning up and saving final state...\")\n","torch.save(model.state_dict(), 'final_model.pt')\n","print(\"Final model state saved\")\n","\n","try:\n","    display_sample_prediction(model, val_loader, device)\n","    print(\"Sample prediction saved as 'sample_prediction.png'\")\n","except Exception as e:\n","    print(f\"Could not generate sample prediction: {str(e)}\")\n","\n","print(\"\\nExecution completed!\")"],"metadata":{"id":"3XFuBtEvIZ35","executionInfo":{"status":"aborted","timestamp":1733601084925,"user_tz":-540,"elapsed":10,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"3XFuBtEvIZ35","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aOGPjAS4Msru","executionInfo":{"status":"aborted","timestamp":1733601084925,"user_tz":-540,"elapsed":9,"user":{"displayName":"Daniil Stepanov","userId":"04922592992140911436"}}},"id":"aOGPjAS4Msru","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}